# 参数管理

在封神框架中，参数通常由各自模型、结构自己定义，一些通用的参数会根据使用的场景也做统一的归类，如果使用者不清楚具体参数的含义以及声明的位置，可能会导致参数重复定义等问题，所以单独拎了一个文档做记录和管理

## 数据相关参数

### UniversalDataModule

该模块是通用的DataModule，通常用于fs_datasets下的数据集，或者数据处理逻辑能用一个函数描述的datasets，都用直接使用该DataModule。
datamodule.datasets是一个dict，结构类似于：

```python
{
    "train": datasets,
    "validation": datasets,
    "test": datasets
}
```

- num_workers：fs_datasets加载数据时的进程数，可以根据CPU的数目配置
- dataloader_workers：dataloader处理数据的进程数，这个配置通常2-4就足够了，配置过大反而会导致处理数据效率降低甚至卡死的情况
- train_batchsize：训练batchsize
- val_batchsize：验证batchsize
- test_batchsize：测试batchsize
- datasets_name：fs_datasets的名字，如果不传入的话则不会加载数据，需要用户显示指定datamodule.datasets=xxxxx
- train_datasets_field：self.datasets中训练集所对应的key
- val_datasets_field：self.datasets中验证集所对应的key（有时val数据集会取用test集，所以设定了这三个参数做兼容
- test_datasets_field：self.datasets中测试集所对应的key
- sampler_type：封神框架中自建的sampler，用于支持大数据集

## 模型相关参数

### add_module_args 通用模型参数

这个函数都是包括了一些常用的、跟特定模型无关的参数。

- learning_rate：学习率
- weight_decay：权重衰减
- warmup_ratio：学习率warmup比例，比如设定是0.1，总步数是100，则前10步会warmup到最大值，并开始衰减
- warmup_steps： 学习率warmup步数，优先级大于warmup_ratio
- adam_beta1：adam参数
- adam_beta2：adam参数
- adam_epsilon：adam参数
- model_path：模型路径
- scheduler_type：支持多种scheduler，包括['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup']

## 训练相关参数

### Lightning Trainer

Lightning Trainer参数可以参考文档[Doc](https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.trainer.trainer.Trainer.html#pytorch_lightning.trainer.trainer.Trainer)

这里列举一些比较常用的：

- max_epochs: 设定总共epoch数
- max_steps：设定总共的steps数，跟max_epochs冲突
- gpus： 机器的gpu数量
- num_nodes：机器数
- strategy：分布式策略，常用的比如ddp, deepspeed_zero_stage_1，所有支持的可以参考lightning文档
- gradient_clip_val：梯度裁剪
- check_val_every_n_epoch：多少个epoch后做一次validation
- val_check_interval：在epoch内做validation的频率，如果是float则是按比例，如果是int型则是按steps算
- precision：模型精度
- default_root_dir：设定日志存放的目录
