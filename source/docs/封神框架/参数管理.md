# 参数管理

## 简介 Brief Introduction

框架中一些常用的参数说明文档。

## 数据相关参数

### UniversalDataModule

该模块是通用的DataModule，通常用于fs_datasets下的数据集，或者数据处理逻辑能用一个函数描述的datasets，都用直接使用该DataModule。
datamodule.datasets是一个dict，结构类似于:

```python
{
    "train":datasets,
    "validation":datasets,
    "test":datasets
}
```

* __num_workers__:fs_datasets加载数据时的进程数，可以根据CPU的数目配置
* __dataloader_workers__:dataloader处理数据的进程数，这个配置通常2-4就足够了，配置过大反而会导致处理数据效率降低甚至卡死的情况
* __train_batchsize__:训练batchsize
* __val_batchsize__:验证batchsize
* __test_batchsize__:测试batchsize
* __datasets_name__:fs_datasets的名字，如果不传入的话则不会加载数据，需要用户显示指定datamodule.datasets=xxxxx
* __train_datasets_field__:self.datasets中训练集所对应的key
* __val_datasets_field__:self.datasets中验证集所对应的key（有时val数据集会取用test集，所以设定了这三个参数做兼容
* __test_datasets_field__:self.datasets中测试集所对应的key
* __sampler_type__:封神框架中自建的sampler，用于支持大数据集

## 模型相关参数

### add_module_args 通用模型参数

这个函数都是包括了一些常用的、跟特定模型无关的参数。

* __learning_rate__:学习率
* __weight_decay__:权重衰减
* __warmup_ratio__:学习率warmup比例，比如设定是0.1，总步数是100，则前10步会warmup到最大值，并开始衰减
* __warmup_steps__: 学习率warmup步数，优先级大于warmup_ratio
* __adam_beta1__:adam参数
* __adam_beta2__:adam参数
* __adam_epsilon__:adam参数
* __model_path__:模型路径
* __scheduler_type__:支持多种scheduler，包括['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup']

## 训练相关参数

### Lightning Trainer

Lightning Trainer参数可以参考文档[Doc](https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.trainer.trainer.Trainer.html#pytorch_lightning.trainer.trainer.Trainer)

这里列举一些比较常用的:

* __max_epochs__:设定总共epoch数
* __max_steps__:设定总共的steps数，跟max_epochs冲突
* __gpus__: 机器的gpu数量
* __num_nodes__:机器数
* __strategy__:分布式策略，常用的比如ddp, deepspeed_zero_stage_1，所有支持的可以参考lightning文档
* __gradient_clip_val__:梯度裁剪
* __check_val_every_n_epoch__:多少个epoch后做一次validation
* __val_check_interval__:在epoch内做validation的频率，如果是float则是按比例，如果是int型则是按steps算
* __precision__:模型精度
* __default_root_dir__:设定日志存放的目录
